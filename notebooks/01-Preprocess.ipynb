{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a526be2d-e18a-4391-ad18-9f0d1f5777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c13f4-576e-48c2-a470-aeda7a04c3c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocess\n",
    "Use the files under the `input/original` folder to generate metadata files under `output` (e.g., `portals-metadata.json`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a17dfc-cca8-4092-aa9f-84ab2b760866",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES = [\n",
    "    (\n",
    "        'portal',                            # category\n",
    "        'database-commons.json',             # input file\n",
    "        'portals-metadata.json',             # output file\n",
    "        'portals-manually-selected.json',    # a list of manually chosen websites. None if `None`\n",
    "    ),\n",
    "    (\n",
    "        'visualization',                     # category\n",
    "        'awesome-genome-visualization.json', # input file\n",
    "        'visualizations-metadata.json',      # output file\n",
    "        None,                                # a list of manually chosen websites. None if `None`\n",
    "    ),\n",
    "    (\n",
    "        'journal',                           # category\n",
    "        'sjr.json',                          # input file\n",
    "        'journals-metadata.json',            # output file\n",
    "        None,                                # a list of manually chosen websites. None if `None`\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea43b5-d53e-4cde-95b9-8db0e703b8bc",
   "metadata": {},
   "source": [
    "## Add Connection Status\n",
    "We want to filter out resources that are no longer working in the evaluation, so add such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342fe1f-3849-43d6-a922-ad54275cd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_status(x):\n",
    "    try:\n",
    "        return requests.get(x).status_code\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return -1\n",
    "\n",
    "def check_webpage(url):\n",
    "    try:\n",
    "        # https://stackoverflow.com/questions/51972160/python-check-if-website-exists-for-a-list-of-websites\n",
    "        conn = urllib.request.urlopen(url, timeout=1)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        return e.code\n",
    "    except urllib.error.URLError as e:\n",
    "        return e.reason\n",
    "    except Exception:\n",
    "        return -1\n",
    "    else:\n",
    "        return 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e76a3-73c0-41c4-a09d-2b779bd09f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using `Sourceid` of SJR, get URLs of individual journal portals\n",
    "\"\"\"\n",
    "def infer_homepage(Sourceid):\n",
    "    info_url = f'https://www.scimagojr.com/journalsearch.php?q={Sourceid}&tip=sid&clean=0'\n",
    "    html_text = requests.get(info_url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    urls = soup.find_all('a', text=re.compile('Homepage'))\n",
    "    if len(urls) > 0:\n",
    "        return urls[0].get('href')\n",
    "    else:\n",
    "        print(f'No homepage found for {Sourceid}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25822c-edc1-4b9f-9233-bc0bb4ca4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (\n",
    "    category, \n",
    "    input_file, \n",
    "    output_file,\n",
    "    manual_file\n",
    ") in RESOURCES:\n",
    "    \n",
    "    input_path = f'../input/{input_file}'\n",
    "    manual_path = f'../input/{manual_file}'\n",
    "    output_path = f'../output/{output_file}'\n",
    "\n",
    "    if os.path.isfile(output_path):\n",
    "        # it looks like there already is an output file\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_json(input_path)\n",
    "    \n",
    "    if category == 'portal':\n",
    "        # actual data is stored under `data`.\n",
    "        df = pd.DataFrame.from_dict(df.data.to_dict(), orient='index')\n",
    "    elif category == 'visualization':\n",
    "        # actual data is stored under `tools`.\n",
    "        df = pd.DataFrame.from_dict(df.tools.to_dict(), orient='index')\n",
    "        df = df[df['platform'].map(lambda x: hasattr(x, \"__len__\") and 'Web' in x)]\n",
    "    elif category == 'journal':\n",
    "        df['url'] = df['Sourceid'].apply(lambda x: infer_homepage(x))\n",
    "        \n",
    "    if manual_file is not None:\n",
    "        # Add manually chosen webpages\n",
    "        manual_selection = pd.read_json(manual_path)\n",
    "        df = df.append(manual_selection)\n",
    "    \n",
    "    df['connection'] = df['url'].apply(lambda x: check_webpage(x))\n",
    "    \n",
    "    # select websites that are able to connect\n",
    "    df = df[df.connection == 200]\n",
    "\n",
    "    df.to_json(output_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d0691-0476-41c5-8754-8defc9e647a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09097bb3-cce9-46f1-b7e1-05f36d539ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
