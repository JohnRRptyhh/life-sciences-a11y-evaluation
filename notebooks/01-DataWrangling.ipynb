{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a526be2d-e18a-4391-ad18-9f0d1f5777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from constants import EVALUATION_DATE_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c13f4-576e-48c2-a470-aeda7a04c3c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Wrangling\n",
    "\n",
    "1. Unify the file formats (i.e., `json` --> `csv`)\n",
    "1. Merge metadata from multiple sources (i.e., NIH Common Fund repositories and Database Commons)\n",
    "1. Add URLs of resources if missing (i.e., Journal homepages using `Sourceid`)\n",
    "1. Add manually collected subpages (e.g., docs, search page, etc)\n",
    "1. Add our own IDs for individual resources by updating (`global_data-portal_id_map.csv`)\n",
    "<!-- 1. Add resource connection status -->\n",
    "\n",
    "**To-Do**\n",
    "- [ ] Merge NIH data portals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the original data\n",
    "\"\"\"\n",
    "df = pd.read_json(os.path.join('../input', EVALUATION_DATE_FOLDER, 'database-commons.json'))\n",
    "\n",
    "# df = df.head(10) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use underscore lowercase column names\n",
    "\"\"\"\n",
    "df.columns = (df.columns.str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop columns that we don't need\n",
    "\"\"\"\n",
    "df.drop(columns=['biodb_ranks', 'rating_list'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The id from the sources are consistently \"source_id\"\n",
    "The values should be a string type, and it has the prefix that represents the source (e.g. dc_ for Database Commons)\n",
    "\"\"\"\n",
    "df.rename(columns={ \"db_id\": \"source_id\" }, inplace=True)\n",
    "df.source_id = df.source_id.apply(lambda x: 'dc_' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Some columns from data commons are in json format, we need to convert them to string\n",
    "Example: [{ \"id\": 1, \"name\": \"foo\" }, { \"id\": 2, \"name\": \"bar\" }] --> 'foo, bar'\n",
    "\"\"\"\n",
    "json_column_names_and_keys = {\n",
    "    'data_type_list': 'datatypeName', \n",
    "    'category_list': 'name',\n",
    "    'keywords_list': 'name',\n",
    "    'data_object_list': 'name',\n",
    "    'organism_list': 'organismName',\n",
    "    'theme_list': 'name'\n",
    "}\n",
    "\n",
    "for (column, key) in json_column_names_and_keys.items():\n",
    "    df[column] = df[column].apply(lambda x: ', '.join([object[key] for object in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes too much time. We will instead just run WAVE API requests for all URLs.\n",
    "# # Check the connection status and put that as a `reachable` column\n",
    "\n",
    "# # TODO: a faster way to do this?\n",
    "# def check_connection_status(url):\n",
    "#     print(url)\n",
    "#     try:\n",
    "#         status = requests.get(url)\n",
    "#     except Exception:\n",
    "#         return False\n",
    "#     return status.status_code == 200\n",
    "    \n",
    "# df['reachable'] = df['url'].apply(lambda x: check_connection_status(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the ID Mapping Table (`data-portal_id_map.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     df_map \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msource_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdate_added\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Find rows that does not already exist in the table\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m df_temp \u001b[39m=\u001b[39m df_map\u001b[39m.\u001b[39mmerge(df, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m'\u001b[39m, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msource_id\u001b[39m\u001b[39m'\u001b[39m, indicator\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df_new_resources \u001b[39m=\u001b[39m df_temp[df_temp[\u001b[39m'\u001b[39m\u001b[39m_merge\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mright_only\u001b[39m\u001b[39m'\u001b[39m][[\u001b[39m'\u001b[39m\u001b[39msource_id\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sehilyi/Documents/a11y/life-sciences-a11y-evaluation/notebooks/01-DataWrangling.ipynb#X13sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Ensure to assign new `id`s, i.e., New `id` == max id + 1\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "create or update a mapping table (i.e., `data-portal_id_map.csv`)\n",
    "\"\"\"\n",
    "# Load the data first\n",
    "file = Path(os.path.join('../output', 'data-portal_id_map.csv'))\n",
    "if file.exists():\n",
    "    df_map = pd.read_csv(file)\n",
    "else:\n",
    "    df_map = pd.DataFrame(columns=['id', 'source_id', 'date_added'])\n",
    "\n",
    "# Find rows that does not already exist in the table\n",
    "df_temp = df_map.merge(df, how='outer', on='source_id', indicator=True)\n",
    "df_new_resources = df_temp[df_temp['_merge'] == 'right_only'][['source_id']]\n",
    "\n",
    "# Ensure to assign new `id`s, i.e., New `id` == max id + 1\n",
    "max_id = df_map.id.max()\n",
    "max_id = 0 if max_id is np.nan else max_id\n",
    "new_id = max_id + 1\n",
    "\n",
    "df_new_resources.insert(0, 'id', range(new_id, new_id + len(df_new_resources)))\n",
    "df_new_resources['date_added'] = pd.to_datetime('today').strftime('%m-%d-%Y')\n",
    "\n",
    "pd.concat([df_map, df_new_resources], axis=0).to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Metadata (`data-portal_metadata.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the `id` column to the original metadata\n",
    "\"\"\"\n",
    "df_map = pd.read_csv(file)\n",
    "df_meta = df_map[['id', 'source_id']].merge(df, how='right', on='source_id')\n",
    "df_meta.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_metadata.csv'), index=False)\n",
    "df_meta.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Homepages (`data-portal_pages.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create `data-portal_pages.csv` and add homepage urls.\n",
    "\"\"\"\n",
    "df_meta = pd.read_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_metadata.csv'))\n",
    "\n",
    "df_pages = df_meta[['id', 'url']].copy()\n",
    "\n",
    "df_pages['page_type'] = 'home'\n",
    "df_pages['page_id'] = df_pages['id']\n",
    "df_pages['page_id'] = df_pages['page_id'].apply(lambda x: str(x) + '_' + 'home')\n",
    "\n",
    "df_pages.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_pages.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_type</th>\n",
       "      <th>page_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>home</td>\n",
       "      <td>1_home</td>\n",
       "      <td>https://david.ncifcrf.gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>home</td>\n",
       "      <td>2_home</td>\n",
       "      <td>https://www.kegg.jp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>home</td>\n",
       "      <td>3_home</td>\n",
       "      <td>https://www.cbioportal.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>home</td>\n",
       "      <td>4_home</td>\n",
       "      <td>https://string-db.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>home</td>\n",
       "      <td>5_home</td>\n",
       "      <td>https://www.encodeproject.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6613</th>\n",
       "      <td>59</td>\n",
       "      <td>documentation</td>\n",
       "      <td>59_documentation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6614</th>\n",
       "      <td>60</td>\n",
       "      <td>search</td>\n",
       "      <td>60_search</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6615</th>\n",
       "      <td>60</td>\n",
       "      <td>search_result</td>\n",
       "      <td>60_search_result</td>\n",
       "      <td>http://smart.embl.de/smart/search.cgi?keywords...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6616</th>\n",
       "      <td>60</td>\n",
       "      <td>data_entity</td>\n",
       "      <td>60_data_entity</td>\n",
       "      <td>http://smart.embl.de/smart/show_motifs.pl?ID=F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6617</th>\n",
       "      <td>60</td>\n",
       "      <td>documentation</td>\n",
       "      <td>60_documentation</td>\n",
       "      <td>http://smart.embl.de/help/FAQ.shtml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6618 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id      page_type           page_id  \\\n",
       "0      1           home            1_home   \n",
       "1      2           home            2_home   \n",
       "2      3           home            3_home   \n",
       "3      4           home            4_home   \n",
       "4      5           home            5_home   \n",
       "...   ..            ...               ...   \n",
       "6613  59  documentation  59_documentation   \n",
       "6614  60         search         60_search   \n",
       "6615  60  search_result  60_search_result   \n",
       "6616  60    data_entity    60_data_entity   \n",
       "6617  60  documentation  60_documentation   \n",
       "\n",
       "                                                    url  \n",
       "0                             https://david.ncifcrf.gov  \n",
       "1                                   https://www.kegg.jp  \n",
       "2                           https://www.cbioportal.org/  \n",
       "3                                https://string-db.org/  \n",
       "4                        https://www.encodeproject.org/  \n",
       "...                                                 ...  \n",
       "6613                                                NaN  \n",
       "6614                                                NaN  \n",
       "6615  http://smart.embl.de/smart/search.cgi?keywords...  \n",
       "6616  http://smart.embl.de/smart/show_motifs.pl?ID=F...  \n",
       "6617                http://smart.embl.de/help/FAQ.shtml  \n",
       "\n",
       "[6618 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add manually collected subpages to `data-portal_pages.csv` from `URL Collection for Subpages - Data Portals.csv`\n",
    "\"\"\"\n",
    "df_subpages = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'URL Collection for Subpages - Data Portals.csv'))\n",
    "\n",
    "# add prefix to the source_id, following our naming convention\n",
    "df_subpages.source_id = df_subpages.source_id.apply(lambda x: 'dc_' + str(x))\n",
    "\n",
    "# Replace the descriptive page_type with the code\n",
    "page_type_map = {\n",
    "    'Home': 'home',\n",
    "    'Search/Filter': 'search',\n",
    "    'Search Result': 'search_result',\n",
    "    'Data Entity (Detail Page)': 'data_entity',\n",
    "    'Documentation or FAQ': 'documentation',\n",
    "}\n",
    "df_subpages.page_type = df_subpages.page_type.apply(lambda x: page_type_map[x])\n",
    "\n",
    "# Append our `id`\n",
    "df_map = pd.read_csv(os.path.join('../output', 'data-portal_id_map.csv'))\n",
    "df_subpages = df_subpages.merge(df_map[['id', 'source_id']], how='left', on='source_id')\n",
    "\n",
    "# Add `page_id` using both `id` and `page_type`\n",
    "df_subpages['page_id'] = df_subpages['id'].astype(str) + '_' + df_subpages['page_type']\n",
    "\n",
    "# Load the existing pages\n",
    "path_pages = os.path.join('../output', EVALUATION_DATE_FOLDER, 'data-portal_pages.csv')\n",
    "df_pages = pd.read_csv(path_pages)\n",
    "\n",
    "# Append the new pags\n",
    "df_pages = df_pages.merge(df_subpages[['id', 'url', 'page_type', 'page_id']], how='outer', on=['page_id', 'page_type', 'id'])\n",
    "\n",
    "# Now that we have two versions of URLs, we prefer to keep the manually collected ones (`url_y`) over the orignal ones (`url_x`) if exist\n",
    "df_pages['url'] = df_pages.url_y.combine_first(df_pages.url_x)\n",
    "\n",
    "# drop the temporary columns\n",
    "df_pages.drop(columns=['url_x', 'url_y'], inplace=True)\n",
    "\n",
    "# save the file\n",
    "df_pages.to_csv(path_pages, index=False)\n",
    "\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Journals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the original data\n",
    "\"\"\"\n",
    "df = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'scimagojr 2022.csv'), sep=';')\n",
    "\n",
    "# df = df.head(5) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using `Sourceid` of SJR, get URLs of individual journal portals\n",
    "TODO: Reuse the previously identified home pages\n",
    "\"\"\"\n",
    "def infer_homepage(Sourceid):\n",
    "    info_url = f'https://www.scimagojr.com/journalsearch.php?q={Sourceid}&tip=sid&clean=0'\n",
    "    html_text = requests.get(info_url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    urls = soup.find_all('a', text=re.compile('Homepage'))\n",
    "    if len(urls) > 0:\n",
    "        return urls[0].get('href')\n",
    "    else:\n",
    "        print(f'No homepage found for {Sourceid}')\n",
    "        return None\n",
    "\n",
    "df['url'] = df['Sourceid'].apply(lambda x: infer_homepage(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We use underscore lowercase column names\n",
    "\"\"\"\n",
    "df.columns = (df.columns.str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True).str.lower())\n",
    "df.columns = (df.columns.str.replace('.', '')) # remove dots\n",
    "df.columns = (df.columns.str.replace('(', '')) # remove parentheses\n",
    "df.columns = (df.columns.str.replace(')', ''))\n",
    "df.columns = (df.columns.str.replace('/', 'per')) # replace slash with \"per\"\n",
    "df.columns = (df.columns.str.replace(' ', '_')) # replace space with underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The id from the sources are consistently \"source_id\"\n",
    "The values should be a string type, and it has the prefix that represents the source (e.g. dc_ for Database Commons)\n",
    "\"\"\"\n",
    "df.rename(columns={ \"sourceid\": \"source_id\" }, inplace=True)\n",
    "df.source_id = df.source_id.apply(lambda x: 'sjr_' + str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the ID Mapping Table (`journal-portal_id_map.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "create or update a mapping table (i.e., `data-portal_id_map.csv`)\n",
    "\"\"\"\n",
    "# Load the data first\n",
    "file = Path(os.path.join('../output', 'journal-portal_id_map.csv'))\n",
    "if file.exists():\n",
    "    df_map = pd.read_csv(file)\n",
    "else:\n",
    "    df_map = pd.DataFrame(columns=['id', 'source_id', 'date_added'])\n",
    "\n",
    "# Find rows that does not already exist in the table\n",
    "df_temp = df_map.merge(df, how='outer', on='source_id', indicator=True)\n",
    "df_new_resources = df_temp[df_temp['_merge'] == 'right_only'][['source_id']]\n",
    "\n",
    "# Ensure to assign new `id`s, i.e., New `id` == max id + 1\n",
    "max_id = df_map.id.max()\n",
    "max_id = 0 if max_id is np.nan else max_id\n",
    "new_id = max_id + 1\n",
    "\n",
    "df_new_resources.insert(0, 'id', range(new_id, new_id + len(df_new_resources)))\n",
    "df_new_resources['date_added'] = pd.to_datetime('today').strftime('%m-%d-%Y')\n",
    "\n",
    "pd.concat([df_map, df_new_resources], axis=0).to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Metadata of Journal Portals (`journal-portal_metadata.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Add the `id` column to the original metadata\n",
    "\"\"\"\n",
    "df_map = pd.read_csv(file)\n",
    "df_meta = df_map[['id', 'source_id']].merge(df, how='right', on='source_id')\n",
    "df_meta.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_metadata.csv'), index=False)\n",
    "df_meta.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Pages of Journal Portals (`journal-portal_pages.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create `*_pages.csv` and add homepage urls.\n",
    "\"\"\"\n",
    "df_meta = pd.read_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_metadata.csv'))\n",
    "\n",
    "df_pages = df_meta[['id', 'url']].copy()\n",
    "\n",
    "df_pages['page_type'] = 'home'\n",
    "df_pages['page_id'] = df_pages['id']\n",
    "df_pages['page_id'] = df_pages['page_id'].apply(lambda x: str(x) + '_' + 'home')\n",
    "\n",
    "df_pages.to_csv(os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_pages.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Subpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>page_type</th>\n",
       "      <th>page_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27956</td>\n",
       "      <td>home</td>\n",
       "      <td>27956_home</td>\n",
       "      <td>https://onlinelibrary.wiley.com/journal/15424863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27957</td>\n",
       "      <td>home</td>\n",
       "      <td>27957_home</td>\n",
       "      <td>https://academic.oup.com/qje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27958</td>\n",
       "      <td>home</td>\n",
       "      <td>27958_home</td>\n",
       "      <td>https://www.nature.com/nrm/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27959</td>\n",
       "      <td>home</td>\n",
       "      <td>27959_home</td>\n",
       "      <td>https://www.cell.com/cell/home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27960</td>\n",
       "      <td>home</td>\n",
       "      <td>27960_home</td>\n",
       "      <td>https://www.nejm.org/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28160</th>\n",
       "      <td>179</td>\n",
       "      <td>home</td>\n",
       "      <td>179_home</td>\n",
       "      <td>https://rupress.org/jem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28161</th>\n",
       "      <td>179</td>\n",
       "      <td>research_article</td>\n",
       "      <td>179_research_article</td>\n",
       "      <td>https://rupress.org/jem/article/221/1/e2023092...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28162</th>\n",
       "      <td>179</td>\n",
       "      <td>none_research_article</td>\n",
       "      <td>179_none_research_article</td>\n",
       "      <td>https://rupress.org/jem/article/221/1/e2023019...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28163</th>\n",
       "      <td>179</td>\n",
       "      <td>article_search_result</td>\n",
       "      <td>179_article_search_result</td>\n",
       "      <td>https://rupress.org/jem/search-results?page=1&amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28164</th>\n",
       "      <td>179</td>\n",
       "      <td>latest_issue</td>\n",
       "      <td>179_latest_issue</td>\n",
       "      <td>https://rupress.org/jem/issue</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28165 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id              page_type                    page_id  \\\n",
       "0      27956                   home                 27956_home   \n",
       "1      27957                   home                 27957_home   \n",
       "2      27958                   home                 27958_home   \n",
       "3      27959                   home                 27959_home   \n",
       "4      27960                   home                 27960_home   \n",
       "...      ...                    ...                        ...   \n",
       "28160    179                   home                   179_home   \n",
       "28161    179       research_article       179_research_article   \n",
       "28162    179  none_research_article  179_none_research_article   \n",
       "28163    179  article_search_result  179_article_search_result   \n",
       "28164    179           latest_issue           179_latest_issue   \n",
       "\n",
       "                                                     url  \n",
       "0       https://onlinelibrary.wiley.com/journal/15424863  \n",
       "1                           https://academic.oup.com/qje  \n",
       "2                            https://www.nature.com/nrm/  \n",
       "3                         https://www.cell.com/cell/home  \n",
       "4                                  https://www.nejm.org/  \n",
       "...                                                  ...  \n",
       "28160                            https://rupress.org/jem  \n",
       "28161  https://rupress.org/jem/article/221/1/e2023092...  \n",
       "28162  https://rupress.org/jem/article/221/1/e2023019...  \n",
       "28163  https://rupress.org/jem/search-results?page=1&...  \n",
       "28164                      https://rupress.org/jem/issue  \n",
       "\n",
       "[28165 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Add manually collected subpages to `journal-portal_pages.csv` from `URL Collection for Subpages - Journals.csv`\n",
    "\"\"\"\n",
    "df_subpages = pd.read_csv(os.path.join('../input', EVALUATION_DATE_FOLDER, 'URL Collection for Subpages - Journals.csv'))\n",
    "\n",
    "# add prefix to the source_id, following our naming convention\n",
    "df_subpages.source_id = df_subpages.source_id.apply(lambda x: 'sjr_' + str(x))\n",
    "\n",
    "# Replace the descriptive page_type with the code\n",
    "page_type_map = {\n",
    "    'Home': 'home',\n",
    "    'Latest Open Access Research Article': 'research_article',\n",
    "    'Latest None-research Article': 'none_research_article',\n",
    "    'Article Search Result': 'article_search_result',\n",
    "    'Latest Issue': 'latest_issue',\n",
    "}\n",
    "df_subpages.page_type = df_subpages.page_type.apply(lambda x: page_type_map[x])\n",
    "\n",
    "# Append our `id`\n",
    "df_map = pd.read_csv(os.path.join('../output', 'journal-portal_id_map.csv'))\n",
    "df_subpages = df_subpages.merge(df_map[['id', 'source_id']], how='left', on='source_id')\n",
    "\n",
    "# Add `page_id` using both `id` and `page_type`\n",
    "df_subpages['page_id'] = df_subpages['id'].astype(str) + '_' + df_subpages['page_type']\n",
    "\n",
    "# Load the existing pages\n",
    "path_pages = os.path.join('../output', EVALUATION_DATE_FOLDER, 'journal-portal_pages.csv')\n",
    "df_pages = pd.read_csv(path_pages)\n",
    "\n",
    "# Append the new pags\n",
    "df_pages = df_pages.merge(df_subpages[['id', 'url', 'page_type', 'page_id']], how='outer', on=['page_id', 'page_type', 'id'])\n",
    "\n",
    "# Now that we have two versions of URLs, we prefer to keep the manually collected ones (`url_y`) over the orignal ones (`url_x`) if exist\n",
    "df_pages['url'] = df_pages.url_y.combine_first(df_pages.url_x)\n",
    "\n",
    "# drop the temporary columns\n",
    "df_pages.drop(columns=['url_x', 'url_y'], inplace=True)\n",
    "\n",
    "# save the file\n",
    "df_pages.to_csv(path_pages, index=False)\n",
    "\n",
    "df_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
