{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "a526be2d-e18a-4391-ad18-9f0d1f5777b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c13f4-576e-48c2-a470-aeda7a04c3c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Metadata Cleaning\n",
    "\n",
    "1. Unify the file formats (i.e., `json` --> `csv`)\n",
    "1. Merge metadata from multiple sources (i.e., NIH Common Fund repositories and Database Commons)\n",
    "1. Add URLs of resources if missing (i.e., Journal homepages using `Sourceid`)\n",
    "1. Add our own IDs for individual resources by updating (`global_data-portal_id_map.csv`)\n",
    "1. Add resource connection status\n",
    "\n",
    "// old \n",
    "Not all existing resources are maintained. We want to first filter out resources that are not working in order to run evaluation with smaller sets of resources.\n",
    "\n",
    "This notebook uses files under the `input` folder (e.g., `database-commons-nov-21-2023.json`) and generates filtered files under `output` (e.g., `portals-filtered-nov-21-2023.json`).\n",
    "\n",
    "**To-Do**\n",
    "- [ ] Merge NIH data portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVALUATION_DATE_FOLDER = 'Nov-21-2023'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Portals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original data\n",
    "df = pd.read_json(os.path.join('../input', EVALUATION_DATE_FOLDER, 'database-commons.json'))\n",
    "\n",
    "df = df.head(10) # for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use underscore lowercase column names\n",
    "df.columns = (df.columns.str.replace('(?<=[a-z])(?=[A-Z])', '_', regex=True).str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that we don't need\n",
    "df.drop(columns=['biodb_ranks', 'rating_list'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The id from the sources are consistently \"source_id\"\n",
    "# The values should be a string type, and it has the prefix that represents the source (e.g. dc_ for Database Commons)\n",
    "df.rename(columns={ \"db_id\": \"source_id\" }, inplace=True)\n",
    "df.source_id = df.source_id.apply(lambda x: 'dc_' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some columns from data commons are in json format, we need to convert them to string\n",
    "# Example: [{ \"id\": 1, \"name\": \"foo\" }, { \"id\": 2, \"name\": \"bar\" }] --> 'foo, bar'\n",
    "json_column_names_and_keys = {\n",
    "    'data_type_list': 'datatypeName', \n",
    "    'category_list': 'name',\n",
    "    'keywords_list': 'name',\n",
    "    'data_object_list': 'name',\n",
    "    'organism_list': 'organismName',\n",
    "    'theme_list': 'name'\n",
    "}\n",
    "\n",
    "for (column, key) in json_column_names_and_keys.items():\n",
    "    df[column] = df[column].apply(lambda x: ', '.join([object[key] for object in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://david.ncifcrf.gov\n",
      "https://www.kegg.jp\n",
      "http://cbioportal.org\n",
      "https://string-db.org/\n",
      "https://www.encodeproject.org/\n",
      "https://www.uniprot.org\n",
      "https://www.internationalgenome.org\n",
      "http://pfam.xfam.org/\n",
      "http://www.arb-silva.de\n",
      "https://gnomad.broadinstitute.org/\n"
     ]
    }
   ],
   "source": [
    "# Check the connection status and put that as a `reachable` column\n",
    "\n",
    "# TODO: a faster way to do this?\n",
    "def check_connection_status(url):\n",
    "    print(url)\n",
    "    try:\n",
    "        status = requests.get(url)\n",
    "    except Exception:\n",
    "        return False\n",
    "    return status.status_code == 200\n",
    "    \n",
    "df['reachable'] = df['url'].apply(lambda x: check_connection_status(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dc_238', 'dc_6934']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create or update a mapping table (i.e., id <==> source_id)\n",
    "\n",
    "# Load the data first\n",
    "file = Path(os.path.join('../output', 'global_data-portal_id_map.csv'))\n",
    "if file.exists():\n",
    "    df_map = pd.read_csv(file)\n",
    "else:\n",
    "    df_map = pd.DataFrame(columns=['id', 'source_id', 'date_added'])\n",
    "\n",
    "# Find rows that does not already existi in the table\n",
    "df_temp = df_map.merge(df, how='outer', on='source_id', indicator=True)\n",
    "df_new_resources = df_temp[df_temp['_merge'] == 'right_only'][['source_id']]\n",
    "\n",
    "# New `id` should be the max `id` + 1\n",
    "max_id = df_map.id.max()\n",
    "new_id = 1 if max_id is np.nan else max_id + 1\n",
    "df_new_resources.insert(0, 'id', range(new_id, new_id + len(df)))\n",
    "df_new_resources['date_added'] = pd.to_datetime('today').strftime('%m-%d-%Y')\n",
    "\n",
    "pd.concat([df_map, df_new_resources], axis=0).to_csv(file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a17dfc-cca8-4092-aa9f-84ab2b760866",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES = [\n",
    "    (\n",
    "        'portal',                            # category\n",
    "        'database-commons.json',             # input file\n",
    "        'portals-metadata.json',             # output file\n",
    "        'portals-manually-selected.json',    # a list of manually chosen websites. None if `None`\n",
    "    ),\n",
    "    (\n",
    "        'journal',                           # category\n",
    "        'sjr.json',                          # input file\n",
    "        'journals-metadata.json',            # output file\n",
    "        None,                                # a list of manually chosen websites. None if `None`\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bea43b5-d53e-4cde-95b9-8db0e703b8bc",
   "metadata": {},
   "source": [
    "## Add Connection Status\n",
    "We want to filter out resources that are no longer working in the evaluation, so add such information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8342fe1f-3849-43d6-a922-ad54275cd746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_status(x):\n",
    "    try:\n",
    "        return requests.get(x).status_code\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        return -1\n",
    "\n",
    "def check_webpage(url):\n",
    "    try:\n",
    "        # https://stackoverflow.com/questions/51972160/python-check-if-website-exists-for-a-list-of-websites\n",
    "        conn = urllib.request.urlopen(url, timeout=1)\n",
    "    except urllib.error.HTTPError as e:\n",
    "        return e.code\n",
    "    except urllib.error.URLError as e:\n",
    "        return e.reason\n",
    "    except Exception:\n",
    "        return -1\n",
    "    else:\n",
    "        return 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49e76a3-73c0-41c4-a09d-2b779bd09f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using `Sourceid` of SJR, get URLs of individual journal portals\n",
    "\"\"\"\n",
    "def infer_homepage(Sourceid):\n",
    "    info_url = f'https://www.scimagojr.com/journalsearch.php?q={Sourceid}&tip=sid&clean=0'\n",
    "    html_text = requests.get(info_url).text\n",
    "    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "    urls = soup.find_all('a', text=re.compile('Homepage'))\n",
    "    if len(urls) > 0:\n",
    "        return urls[0].get('href')\n",
    "    else:\n",
    "        print(f'No homepage found for {Sourceid}')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25822c-edc1-4b9f-9233-bc0bb4ca4916",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (\n",
    "    category, \n",
    "    input_file, \n",
    "    output_file,\n",
    "    manual_file\n",
    ") in RESOURCES:\n",
    "    \n",
    "    input_path = f'../input/{input_file}'\n",
    "    manual_path = f'../input/{manual_file}'\n",
    "    output_path = f'../output/{output_file}'\n",
    "\n",
    "    if os.path.isfile(output_path):\n",
    "        # it looks like there already is an output file\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_json(input_path)\n",
    "    \n",
    "    if category == 'portal':\n",
    "        # actual data is stored under `data`.\n",
    "        df = pd.DataFrame.from_dict(df.data.to_dict(), orient='index')\n",
    "    elif category == 'visualization':\n",
    "        # actual data is stored under `tools`.\n",
    "        df = pd.DataFrame.from_dict(df.tools.to_dict(), orient='index')\n",
    "        df = df[df['platform'].map(lambda x: hasattr(x, \"__len__\") and 'Web' in x)]\n",
    "    elif category == 'journal':\n",
    "        df['url'] = df['Sourceid'].apply(lambda x: infer_homepage(x))\n",
    "        \n",
    "    if manual_file is not None:\n",
    "        # Add manually chosen webpages\n",
    "        manual_selection = pd.read_json(manual_path)\n",
    "        df = df.append(manual_selection)\n",
    "    \n",
    "    df['connection'] = df['url'].apply(lambda x: check_webpage(x))\n",
    "    \n",
    "    # select websites that are able to connect\n",
    "    df = df[df.connection == 200]\n",
    "\n",
    "    df.to_json(output_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d0691-0476-41c5-8754-8defc9e647a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09097bb3-cce9-46f1-b7e1-05f36d539ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
